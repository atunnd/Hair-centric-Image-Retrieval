# Baseline DualViewHair Configuration
# Standard teacher-student architecture with InfoNCE loss

# Model Configuration
model:
  encoder_type: "baseline"          # baseline, multiscale, partbased
  embedding_dim: 256                # Feature embedding dimension
  projection_dim: 128               # Contrastive projection dimension
  momentum: 0.999                   # Teacher momentum update rate
  pretrained: true                  # Use ImageNet pretrained backbone
  use_cross_alignment: false        # Enable cross-view alignment

# Loss Configuration
loss:
  loss_type: "infonce"              # infonce, ntxent, asymmetric_ntxent, hybrid
  temperature: 0.1                  # Contrastive learning temperature
  # Additional parameters for specific losses
  # weight_s2t: 1.0                 # Student-to-teacher weight (asymmetric_ntxent)
  # weight_t2s: 0.5                 # Teacher-to-student weight (asymmetric_ntxent)
  # alignment_weight: 0.1           # Cross-view alignment weight (hybrid)

# Data Configuration
data:
  data_root: "./data"               # Root directory containing hair_regions and full_images
  image_size: 224                   # Input image size (square)
  normalize_mean: [0.485, 0.456, 0.406]  # ImageNet normalization
  normalize_std: [0.229, 0.224, 0.225]

# Training Configuration
training:
  num_epochs: 50                    # Number of training epochs
  batch_size: 32                    # Training batch size
  learning_rate: 0.001              # Initial learning rate
  weight_decay: 0.0001              # L2 regularization
  optimizer: "adam"                 # adam, adamw, sgd
  scheduler: "cosine"               # cosine, step, warmup_cosine, null
  warmup_epochs: 5                  # Warmup epochs (for warmup_cosine)
  grad_clip_norm: 1.0               # Gradient clipping norm (0 to disable)
  
  # Logging and saving
  log_interval: 100                 # Log every N batches
  save_interval: 10                 # Save checkpoint every N epochs
  output_dir: "./outputs/baseline"  # Output directory for checkpoints and logs

# Evaluation Configuration
evaluation:
  batch_size: 64                    # Evaluation batch size
  top_k: [1, 5, 10, 20]            # Top-K values for evaluation
  metrics: ["map", "recall"]        # Metrics to compute
  use_teacher: true                 # Use teacher encoder for evaluation
