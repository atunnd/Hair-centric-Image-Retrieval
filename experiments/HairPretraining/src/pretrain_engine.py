import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
from tqdm import tqdm
import os
import math

import lightly
from lightly.loss import NTXentLoss
from utils.utils import get_optimizer, linear_increase_alpha, margin_decay, mse_alignment_loss, get_latest_checkpoint, sample_random_hard_negatives
from utils.transform import positive_transform, negative_transform, PositiveMaskingTransform

from utils.losses import positive_consistency_loss_margin, bidirectional_margin_loss, nt_xent_1anchor_2positive

#from utils.losses import DINOLoss, IBOTPatchLoss
from .neg_sampling import NegSamplerClasses, NegSamplerRandomly, NegSamplerNN, NegSamplerStatic
import timm
from lightly.utils.scheduler import cosine_schedule
from lightly.models.utils import deactivate_requires_grad, update_momentum
from torch.utils.tensorboard import SummaryWriter
import torch.nn.functional as F
from lightly.loss import KoLeoLoss
from lightly.models.utils import (
    random_block_mask,
    update_drop_path_rate,
    update_momentum,
)
from lightly.utils.scheduler import cosine_schedule, linear_warmup_schedule
import random

import faiss
import numpy as np

class Trainer:
    def __init__(self, model, train_loader, val_loader, args):

        # load model
        self.model = model.to(args.device)
        # load dataloader
        self.train_loader = train_loader
        # training setting
        self.epochs = args.epochs
        self.device = args.device
        self.lr = args.lr
        self.weight_decay = args.weight_decay
        self.beta1 = args.beta1
        self.beta2 = args.beta2
        self.device_id = args.device_id
        self.args = args
        self.save_path = args.save_path
        self.start_epoch = 0
        # choosing mode
        self.mode = args.mode
        self.momentum_ema = args.ema
        self.scaler = torch.cuda.amp.GradScaler() 
        
        # memory for hard negative
        self.hard_negative_memory = []
        self.warm_up_epochs = args.warm_up_epochs
        self.sampling_frequency = args.sampling_frequency


        ##########################################
        #    Setting loss function for each mode #
        ##########################################
        if self.mode == 'mae':
            self.criterion = nn.MSELoss()
        elif self.mode == 'simclr':
            self.criterion = NTXentLoss(temperature=args.loss_temp)
        elif self.mode == 'simclr_supcon':
            self.criterion = SupConLoss()
        elif self.mode == "dinov2":
            #self.criterion = DINOLoss(output_dim=2048,warmup_teacher_temp_epochs=5,)
            self.criterion1 = DINOLoss()
            self.criterion2 = IBOTPatchLoss()
            self.criterion3 = KoLeoLoss()
            self.criterion = "Total loss DINO, IBOTPatchLoss, KoLeoLoss"
        elif self.mode == "simMIM":
            self.criterion = nn.L1Loss()
        elif self.mode == "SHAM":
            self.criterion1 = NTXentLoss(temperature=0.5)
            self.criterion2 = positive_consistency_loss_margin
            self.criterion3 = bidirectional_margin_loss
            self.criterion4 = nn.MSELoss()
        
        # optimizer configuration
        self.optimizer = get_optimizer(self.model, self.lr, self.weight_decay, self.beta1, self.beta2)

        # choosing backbone
        self.mode_model = args.model

        ####################################
        #        Loading checkpoint        #
        ####################################
        if self.args.continue_training:
            try:
                latest_ckpt_path = get_latest_checkpoint(args.checkpoint_folder)
                checkpoint = torch.load(latest_ckpt_path, map_location=self.device, weights_only=False)
                self.save_path = args.checkpoint_folder
                print(f"‚úÖ Found checkpoint: {latest_ckpt_path}")
            except (FileNotFoundError, TypeError):
                print("‚ö†Ô∏è No valid checkpoint found, starting from scratch.")
                self.start_epoch = 0
                global_loss, local_loss = 0.0, 0.0
            else:
                # Load model weights
                self.model.load_state_dict(checkpoint['model_state_dict'])

                # Load scaler
                self.scaler.load_state_dict(checkpoint['scaler_state_dict'])

                # Kh·ªüi t·∫°o optimizer m·ªõi kh·ªõp v·ªõi model hi·ªán t·∫°i
                self.optimizer = get_optimizer(
                    self.model,
                    self.args.lr,
                    self.args.weight_decay,
                    self.args.beta1,
                    self.args.beta2
                )

                # Load optimizer state
                self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])

                # Load epoch v√† c√°c th√¥ng tin b·ªï sung
                self.start_epoch = checkpoint.get('epoch', 0)
                global_loss = checkpoint.get('global_loss', 0.0)
                local_loss = checkpoint.get('local_loss', 0.0)

                print(f"üîÅ Loaded checkpoint from epoch {self.start_epoch}")

                # ƒê·∫£m b·∫£o optimizer tr√™n ƒë√∫ng device (ƒë·∫∑c bi·ªát khi map_location='cpu')
                for state in self.optimizer.state.values():
                    for k, v in state.items():
                        if isinstance(v, torch.Tensor):
                            state[k] = v.to(self.device)
        else:
            self.start_epoch = 0
            global_loss, local_loss = 0.0, 0.0


        ####################################
        #    Creating saving directory     #
        ####################################    
        self.momentum_ema = args.ema
        if args.mode=="SHAM":
            self.save_path = os.path.join(self.save_path, f"{self.mode}_{self.mode_model}_{self.args.SHAM_mode}")    
        else: 
            self.save_path = os.path.join(self.save_path, f"{self.mode}_{self.mode_model}")
        if not os.path.exists(self.save_path) and self.args.continue_training is False:
            print(f"Save {args.mode} at {self.save_path}")      
            os.makedirs(self.save_path, exist_ok=True)
            new_log=True
        else:
            new_log=False


        mode = 'a' if not new_log else 'w'
        self.log_file = os.path.join(self.save_path, 'training_log.txt')
        with open(self.log_file, mode) as f: 
            if new_log:
                f.write("Training Log - Loss per Epoch\n")
            else:
                f.write("\n---- Resume training ----\n")

        self.log_dir = os.path.join(self.save_path, "logs")
        os.makedirs(self.log_dir, exist_ok=True)
        self.writer = SummaryWriter(log_dir=self.log_dir) 
    
    def train_one_epoch_simclr(self, epoch=0, alpha=0, scaler=None):
        self.model.train()
        running_loss = 0.0
        for batch in tqdm(self.train_loader, desc="Training"):
            with torch.amp.autocast(device_type="cuda", dtype=torch.float16):
                images = batch[0]
                x0, x1 = images[0], images[1]
                x0 = x0.to(self.device)
                x1 = x1.to(self.device)
                z0 = self.model(x0)
                z1 = self.model(x1)

                loss = self.criterion(z0, z1)
                running_loss += loss.detach()

            #loss.backward()
            scaler.scale(loss).backward()
            #self.optimizer.step()
            scaler.step(self.optimizer)
            scaler.update()
            self.optimizer.zero_grad() 
        return running_loss / len(self.train_loader)
    
    def train_one_epoch_mae(self, epoch=0, alpha=0, scaler=None):
        self.model.train()
        running_loss = 0.0
        for batch in tqdm(self.train_loader, desc="Training"):
            with torch.amp.autocast(device_type="cuda", dtype=torch.float16):
                views = batch[0]
                images = views[0].to(self.device)
                predictions, targets = self.model(images)
                loss = self.criterion(predictions, targets)
                running_loss += loss.detach()
            #loss.backward()
            scaler.scale(loss).backward()
            #self.optimizer.step()
            scaler.step(self.optimizer)
            scaler.update()
            self.optimizer.zero_grad()

        return running_loss / len(self.train_loader)

    def train_one_epoch_simclr_supcon(self, epoch=0, alpha=0, scaler=None):
        self.model.train()
        running_loss = 0.0
        for batch in tqdm(self.train_loader, desc="Training with simclr on supcon"):
            with torch.amp.autocast(device_type="cuda", dtype=torch.float16):
                images, labels = batch[0], batch[1]
                images = [img.to(self.device) for img in images]
                labels = labels.to(self.device)
                images = torch.cat([images[0], images[1]], dim=0)
                bsz = labels.shape[0]
                
                features = self.model(images)
                f1, f2 = torch.split(features, [bsz, bsz], dim=0)
                features = torch.cat([f1.unsqueeze(1), f2.unsqueeze(1)], dim=1)
                loss = self.criterion(features, labels)
                running_loss += loss.detach()

            #loss.backward()
            scaler.scale(loss).backward()
            #self.optimizer.step()
            scaler.step(self.optimizer)
            scaler.update()
            self.optimizer.zero_grad()

        return running_loss / len(self.train_loader)
    
    def train_one_epoch_dinov2(self, epoch=0, alpha=0, scaler=None):
        self.model.train()
        running_loss = 0.0
        self.total_steps = self.epochs * len(self.train_loader)
        for batch_idx, batch in enumerate(tqdm(self.train_loader, desc="Training with dinov2")):
            views = batch[0]
            views = [view.to(self.device) for view in views]
            global_views = torch.cat(views[:2])
            local_views = torch.cat(views[2:])

            # Masking
            B = len(global_views)
            sequence_length = self.model.teacher_backbone.sequence_length
            mask = global_views.new_zeros((B, sequence_length), dtype=torch.bool)

            H, W = self.model.teacher_backbone.vit.patch_embed.grid_size
            assert (
                H * W == sequence_length - 1
            ), f"Unexpected grid size: {H}x{W}, sequence_length {sequence_length}"

            block_mask = random_block_mask(size=(B, H, W), device=mask.device)
            mask[:, 1:] = block_mask.flatten(start_dim=1)

            with torch.cuda.amp.autocast(dtype=torch.float16):
                # Teacher forward
                with torch.no_grad():
                    teacher_cls_token, teacher_features = self.model.forward_teacher(global_views)
                    teacher_cls_out = self.model.teacher_head.dino_head.forward(teacher_cls_token)
                    teacher_masked_out = self.model.teacher_head.ibot_head.forward(
                        teacher_features[mask]
                    )

                # Student forward
                student_global_cls_token, student_global_masked_features = \
                    self.model.forward_student(global_views, mask=mask)

                student_global_cls_out = self.model.student_head.dino_head.forward(
                    student_global_cls_token
                )
                student_global_masked_out = self.model.student_head.ibot_head.forward(
                    student_global_masked_features
                )
                student_local_cls_token, _ = self.model.forward_student(local_views, mask=None)
                student_local_cls_out = self.model.student_head.dino_head.forward(
                    student_local_cls_token
                )
                student_cls_out = torch.cat([student_global_cls_out, student_local_cls_out])

                # Loss
                global_step = epoch * len(self.train_loader) + batch_idx
                teacher_temp = linear_warmup_schedule(
                    step=global_step,
                    warmup_steps=int(30 / self.epochs * self.total_steps),
                    start_value=0.04,
                    end_value=0.07,
                )
                dino_loss = self.criterion1(
                    teacher_out=teacher_cls_out.chunk(2),
                    student_out=student_cls_out.chunk(len(views)),
                    teacher_temp=teacher_temp,
                )
                ibot_loss = self.criterion2(
                    teacher_out=teacher_masked_out,
                    student_out=student_global_masked_out,
                    mask=block_mask,
                    teacher_temp=teacher_temp,
                )
                koleo_loss = 0.1 * sum(
                    self.criterion3(t) for t in student_global_cls_token.chunk(2)
                )
                loss = dino_loss + ibot_loss + koleo_loss

            running_loss += loss.detach()

            # ‚úÖ Mixed Precision update
            scaler.scale(loss).backward()

            # zero lr for last layer if needed
            if epoch < 1:
                for param_group in self.optimizer.param_groups:
                    if "last_layer" in param_group:
                        param_group["lr"] = 0.0

            # weight decay schedule
            weight_decay = cosine_schedule(
                step=global_step,
                max_steps=self.total_steps,
                start_value=0.04,
                end_value=0.4,
            )
            for group in self.optimizer.param_groups:
                if group["weight_decay"] != 0.0:
                    group["weight_decay"] = weight_decay

            scaler.step(self.optimizer)
            scaler.update()
            self.optimizer.zero_grad()

            # momentum update teacher
            momentum = cosine_schedule(
                step=global_step,
                max_steps=self.total_steps,
                start_value=0.992,
                end_value=1.0,
            )
            update_momentum(self.model.student_backbone, self.model.teacher_backbone, m=momentum)
            update_momentum(self.model.student_head, self.model.teacher_head, m=momentum)

        avg_loss = running_loss / len(self.train_loader)
        print(f"epoch: {epoch:>02}, loss: {avg_loss:.5f}")
        return avg_loss

    def train_one_epoch_simMIM(self, epoch=0, alpha=0, scaler=None):
        running_loss =0.0
        for batch in tqdm(self.train_loader, desc="Training with simMIM"):
            with torch.amp.autocast(device_type="cuda", dtype=torch.float16):
                views = batch[0]
                images = views[0].to(self.device)  # views contains only a single view
                predictions, targets = self.model(images)

                loss = self.criterion(predictions, targets)
                running_loss += loss.detach()

            #loss.backward()
            scaler.scale(loss).backward()
            #self.optimizer.step()
            scaler.step(self.optimizer)
            scaler.update()
            self.optimizer.zero_grad()
        
        return running_loss/len(self.train_loader)
    
    # -----------------------------
    # Step 1. Estimate K by PCA (SCAN-style)
    # -----------------------------
    def estimate_K_by_PCA(self, X: torch.Tensor, explained_var_threshold=0.9, scale_factor=2.0, max_K=2000):
        """∆Ø·ªõc l∆∞·ª£ng s·ªë c·ª•m K b·∫±ng PCA cumulative variance ratio"""
        X_np = X.detach().cpu().numpy().astype('float32')
        N, D = X_np.shape
        n_components = min(D, N - 1)
        pca = faiss.PCAMatrix(D, n_components)
        pca.train(X_np)
        eigenvalues = faiss.vector_to_array(pca.eigenvalues).astype('float32')
        explained_ratio = eigenvalues / np.sum(eigenvalues)
        cumulative_ratio = np.cumsum(explained_ratio)
        m_star = np.searchsorted(cumulative_ratio, explained_var_threshold) + 1
        K_est = int(np.clip(scale_factor * m_star, 5, min(max_K, N - 1)))
        return K_est, m_star


    # -----------------------------
    # Step 2. K-Means clustering
    # -----------------------------
    def run_kmeans(self, X: torch.Tensor, K: int, device_id: int = 0):
        """Ch·∫°y K-means b·∫±ng FAISS, tr·∫£ v·ªÅ centroids tensor""" 
        X_np = X.detach().cpu().numpy().astype('float32') 
        D = X_np.shape[1] 
        use_gpu = faiss.get_num_gpus() > 0 
        kmeans = faiss.Kmeans(d=D, k=K, niter=50, gpu=False, verbose=True) 
        kmeans.train(X_np) 
        centroids = torch.from_numpy(kmeans.centroids).to(X.device) 
        return centroids, kmeans

    # -----------------------------
    # Step 3. Hard negative mining (cluster-based)
    # -----------------------------
    def mine_hard_negatives(self, anchor: torch.Tensor, centroids: torch.Tensor, kmeans):
        """
        Ch·ªçn hard negatives cho t·ª´ng anchor b·∫±ng c√°ch:
        - t√¨m 2 centroids g·∫ßn nh·∫•t
        - d√πng centroid th·ª© 2 (neighbor) ƒë·ªÉ t√¨m c√°c m·∫´u g·∫ßn n√≥ nh·∫•t
        """
        device = anchor.device
        N, D = anchor.shape

        # --- 1. T√¨m 2 centroid g·∫ßn nh·∫•t ---
        D_c, I_c = kmeans.index.search(anchor.detach().cpu().numpy().astype('float32'), 2)
        neighbor_centroid_ids = torch.from_numpy(I_c[:, 1]).long().to(device)  # (N,)

        # --- 2. Build FAISS index t·ª´ c√°c sample ---
        index = faiss.IndexFlatL2(D)
        index.add(anchor.detach().cpu().numpy().astype('float32'))

        # --- 3. T√¨m top-k sample g·∫ßn t·ª´ng centroid ---
        topk = 5
        D_samp, I_samp = index.search(centroids.detach().cpu().numpy().astype('float32'), topk)
        I_samp = torch.from_numpy(I_samp).long().to(device)  # (K, topk)

        # --- 4. Ch·ªçn hard negative cho t·ª´ng anchor ---
        rand_offsets = torch.randint(0, topk, (N,), device=device)
        hard_neg_ids = I_samp[neighbor_centroid_ids, rand_offsets]

        # Tr√°nh ch·ªçn ch√≠nh anchor ‚Üí thay b·∫±ng ph·∫ßn t·ª≠ th·ª© 0 n·∫øu tr√πng
        same_mask = hard_neg_ids == torch.arange(N, device=device)
        hard_neg_ids[same_mask] = I_samp[neighbor_centroid_ids[same_mask], 0]

        # --- 5. L·∫•y embeddings ---
        #hard_negatives = anchor[hard_neg_ids]
        return hard_neg_ids
    
    def train_one_epoch_SHAM(self, epoch=0, momentum_val=0.99, scaler=None):
        self.model.train()
        running_loss_total = 0.0
        running_loss_contrastive = 0.0
        running_loss_pos_pos = 0.0
        running_loss_margin = 0.0
        running_loss_reconstruction =0.0

        for batch_id, batch in enumerate(tqdm(self.train_loader, desc="Training with negative samples")):
            self.optimizer.zero_grad()
            images = batch
            current_m = momentum_val 
        
            update_momentum(self.model.backbone, self.model.teacher_backbone, m=current_m)
            update_momentum(self.model.proj_head, self.model.teacher_proj_head, m=current_m)
            
            with torch.amp.autocast(device_type="cuda", dtype=torch.float16):
                x_anchor = images['anchor'].to(self.device)
                x_pos_1 = images['pos1'].to(self.device) 
                x_pos_2 = images['pos2'].to(self.device)
                
                res = self.model(img_anchor=x_anchor, img_pos1=x_pos_1, img_pos2=x_pos_2)
                embedding_anchor, embedding_pos1, embedding_pos2, masked_prediction, masked_GT = res['anchor'], res['pos1'], res['pos2'], res['masked_prediction'], res['masked_GT']
                
                if (epoch + 1) >= self.warm_up_epochs:
                    #if (epoch+1 - self.warm_up_epochs) % self.sampling_frequency == 0:
                    if (epoch+1) - self.warm_up_epochs == 0:
                        #K, m_star = self.estimate_K_by_PCA(embedding_anchor)
                        if batch_id ==0:
                            print("=> Sampling new cluster\n")
                            self.hard_negative_memory.clear()
                        K=6
                        centroids, kmeans = self.run_kmeans(embedding_anchor, K, self.device_id)
                        hard_neg_ids = self.mine_hard_negatives(embedding_anchor, centroids, kmeans)
                        self.hard_negative_memory.append(hard_neg_ids.detach().cpu().numpy())
                        #print(f"Estimated K = {K} (m* = {m_star})")
                    else:
                        hard_neg_ids = self.hard_negative_memory[batch_id]
                else:
                    hard_neg_ids = sample_random_hard_negatives(embedding_anchor)
                    if epoch == 0:
                        self.hard_negative_memory.append(hard_neg_ids.detach().cpu().numpy())
                    else:
                        self.hard_negative_memory[batch_id] = hard_neg_ids.detach().cpu().numpy()

                embedding_hard_negative = embedding_anchor[hard_neg_ids]
                
                contrastive_loss = self.criterion1(embedding_anchor, embedding_pos1) # contrastive loss
                pos_consistency_loss = self.criterion2(embedding_pos1, embedding_pos2) # Positive‚Äìpositive consistency loss
                bidirectional_margin_loss = self.criterion3(embedding_anchor, embedding_pos1, embedding_pos2, embedding_hard_negative) 
                reconstruction_loss = self.criterion4(masked_prediction, masked_GT) 
                
                total_loss = contrastive_loss + 0.5*reconstruction_loss + 0.3*bidirectional_margin_loss + 0.1*pos_consistency_loss
            
            running_loss_total += total_loss.item()
            running_loss_contrastive += contrastive_loss.item()
            running_loss_pos_pos += pos_consistency_loss.item()
            running_loss_margin += bidirectional_margin_loss.item()
            running_loss_reconstruction += reconstruction_loss.item()
            
            #total_loss.backward()
            #self.optimizer.step()
            scaler.scale(total_loss).backward()
            scaler.unscale_(self.optimizer)
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
            scaler.step(self.optimizer)
            scaler.update()


            del (
                x_anchor, x_pos_1, x_pos_2,
                embedding_anchor, embedding_pos1, embedding_pos2,
                embedding_hard_negative,
                masked_prediction, masked_GT,
                total_loss, contrastive_loss, pos_consistency_loss,
                bidirectional_margin_loss, reconstruction_loss
            )
        
        self.writer.add_scalar('Epoch/Current', epoch, global_step=epoch)
        self.writer.add_scalars(
            'Loss/Avg_per_Epoch',  # Nh√≥m d∆∞·ªõi tag n√†y ƒë·ªÉ d·ªÖ xem theo epoch
            {
                'total_loss': running_loss_total/len(self.train_loader),
            },
            global_step=epoch  # S·ª≠ d·ª•ng epoch tr·ª±c ti·∫øp l√†m step cho log epoch-level
        )

        with open(self.log_file, 'a') as f:
            f.write(f"\nEpoch {epoch}: Total Loss = {running_loss_total/len(self.train_loader):.6f}, Contrastive Loss = {running_loss_contrastive/len(self.train_loader):.6f}, Pos-pos Loss = {running_loss_pos_pos/len(self.train_loader):.6f}, Bi-margin Loss = {running_loss_margin/len(self.train_loader):.6f}, Reconstruction loss = {running_loss_reconstruction/len(self.train_loader):.6f}\n")

        return running_loss_total/len(self.train_loader), running_loss_contrastive/len(self.train_loader), running_loss_pos_pos/len(self.train_loader), running_loss_margin/len(self.train_loader), running_loss_reconstruction/len(self.train_loader)
    
    def train(self):
        if self.mode == "mae":
            train_one_epoch = self.train_one_epoch_mae
        elif self.mode == 'simclr':
            train_one_epoch = self.train_one_epoch_simclr
        elif self.mode == 'simclr_supcon':
            train_one_epoch = self.train_one_epoch_simclr_supcon
        elif self.mode == "dinov2":
            train_one_epoch = self.train_one_epoch_dinov2
        elif self.mode == "simMIM":
            train_one_epoch = self.train_one_epoch_simMIM
        elif self.mode == "SHAM":
            train_one_epoch = self.train_one_epoch_SHAM


        for epoch in range(self.start_epoch, self.epochs):
            print(f"Epoch {epoch}/{self.epochs}")
            if self.mode=="SHAM":
                total_loss, contrastive_loss, pos_pos_loss, bi_margin_loss, reconstruction_loss = train_one_epoch(epoch=epoch, momentum_val=self.momentum_ema, scaler=self.scaler)
                print(f"Total train loss: {total_loss:.6f}, Contrastive Loss: {contrastive_loss:.6f}, Pos-pos Loss: {pos_pos_loss:.6f}, Bi-margin Loss: {bi_margin_loss:.6f}, Reconstruction Loss: {reconstruction_loss:.6f}")
            else:
                train_loss = train_one_epoch(epoch=epoch, alpha=0, scaler=self.scaler)
                print(f"Train loss: {train_loss:.4f}")
            if (epoch+1) % 50 == 0:
                file_name = os.path.join(self.save_path, f"model_ckpt_{epoch}.pth")
                checkpoint = {
                    'epoch': epoch,
                    'model_state_dict': self.model.state_dict(),
                    'optimizer_state_dict': self.optimizer.state_dict(),
                    'scaler_state_dict': self.scaler.state_dict(),
                    'args': self.args,
                    "Total_loss": total_loss,
                    'Contrastive Loss': contrastive_loss,
                    'Pos-pos loss': pos_pos_loss,
                    'Bi-margin loss': bi_margin_loss,
                    'Reconstruction loss': reconstruction_loss
                }
                torch.save(checkpoint, file_name)
                print(f"‚úÖ Saved checkpoint at epoch {epoch} -> {file_name}")
            file_name = os.path.join(self.save_path, f"model_ckpt_latest.pth")
            checkpoint = {
                'epoch': epoch,
                'model_state_dict': self.model.state_dict(),
                'optimizer_state_dict': self.optimizer.state_dict(),
                'scaler_state_dict': self.scaler.state_dict(),
                'args': self.args,
                "Total_loss": total_loss,
                'Contrastive Loss': contrastive_loss,
                'Pos-pos loss': pos_pos_loss,
                'Bi-margin loss': bi_margin_loss,
                'Reconstruction loss': reconstruction_loss
            }
            torch.save(checkpoint, file_name)
            print(f"‚úÖ Saved checkpoint at epoch {epoch} -> {file_name}")

        self.writer.close()  # Gi·∫£i ph√≥ng resource
            